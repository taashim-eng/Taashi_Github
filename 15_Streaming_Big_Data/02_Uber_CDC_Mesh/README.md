# Uber Style: CDC Data Mesh (Change Data Capture)

## 1. Problem Statement
In a massive monolith (like Uber's early days), querying the main database for analytics or driver incentives slows down the app for everyone. We need to 'stream' database changes instantly to microservices without hitting the main database with heavy queries.

## 2. Requirements & KPIs
- Uncouple Microservices: The Incentive Service should not query the Main DB directly.
- Real-Time: Database changes (Inserts/Updates) must propagate in < 500ms.
- Event Format: Use standard CDC format (Operation, Before, After).
- Business Logic: Instantly trigger 'Bonus Unlocked' events when thresholds are met.

## 3. Architecture & Pipeline
1. **Source (Monolith)**: PostgreSQL Database (Users, Trips).
2. **Capture (Producer)**: Simulates Debezium reading the Write-Ahead Log (WAL).
3. **Transport**: Kafka Topic `db-changes.public.drivers`.
4. **Sink (Consumer)**: Incentive Service builds a local 'Mesh' state to calculate bonuses.

---

## 4. Technical Implementation

### File Structure
- `producer.py`: Simulates the CDC Process (Debezium) emitting WAL events.
- `consumer.py`: The Downstream Microservice (Incentives) consuming the stream.
- `utils_logger.py`: Logging utility.
- `.env`: Environment config.

### Architecture Diagram: Uber CDC Data Mesh

```mermaid
%%{init: {'theme': 'neutral', 'themeVariables': { 'fontFamily': 'arial', 'fontSize': '14px'}}}%%
graph TD
    %% Definitions & Styling
    classDef source fill:#E1D5E7,stroke:#9673A6,stroke-width:2px,color:#000;
    classDef ingestion fill:#FFF2CC,stroke:#D6B656,stroke-width:2px,color:#000;
    classDef processing fill:#DAE8FC,stroke:#6C8EBF,stroke-width:2px,color:#000;
    classDef storage fill:#F5F5F5,stroke:#666666,stroke-width:2px,color:#000;
    classDef action fill:#D5E8D4,stroke:#82B366,stroke-width:2px,color:#000;

    subgraph Source ["Legacy Monolith"]
        Postgres[("Postgres DB<br/>(Users & Trips Tables)")]:::source
        WAL["Write-Ahead Log<br/>(Transaction History)"]:::source
    end

    subgraph CDC ["Change Data Capture"]
        Debezium["CDC Producer<br/>(Simulates Debezium)"]:::ingestion
    end

    subgraph Transport ["Event Backbone"]
        Kafka["Apache Kafka<br/>Topic: db-changes"]:::ingestion
    end

    subgraph MeshNode ["Incentive Microservice (Data Mesh Node)"]
        Consumer["Incentive Service<br/>(Python Consumer)"]:::processing
        LocalCache[("Local State Cache<br/>(Driver Stats)")]:::storage
    end

    subgraph Outcome ["Business Value"]
        Bonus["Bonus Triggered<br/>(Real-Time Payout)"]:::action
    end

    %% Data Flow
    Postgres -->|"1. Transaction Commit"| WAL
    WAL -->|"2. Read Log"| Debezium
    Debezium -->|"3. Publish CDC Event<br/>(Insert/Update)"| Kafka
    Kafka -->|"4. Consume Change"| Consumer

    Consumer -->|"5. Update Local State"| LocalCache
    LocalCache -.->|"6. Check Logic<br/>(Trips > 15)"| Consumer
    Consumer -->|"7. Unlock Bonus"| Bonus

    linkStyle 2,3,4,5,6 stroke:#007ACC,stroke-width:2px,fill:none;
```




### How to Run this Demo

**Step 1: Install Dependencies**
```bash
pip install -r requirements.txt
```

**Step 2: Start the Microservice (Consumer)**
This service acts as a Data Mesh node. It listens for DB changes to update its local cache.
```bash
python consumer.py
```
*It will start waiting for database events...*

**Step 3: Start the Database Simulator (Producer)**
This script generates random Insert/Update/Delete events as if users are taking trips.
```bash
python producer.py
```

**Step 4: Observe Real-Time Sync**
Watch the Consumer terminal. You will see:
- ðŸ”„ **Syncing Trip Data**: Updates the local cache.
- â­ **Rating Update**: Updates driver scores.
- ðŸ’° **BONUS UNLOCKED**: When logic (Trips > 15 & Rating > 4.8) is met.

---
*Generated by Automation Script | Uber Style: CDC Data Mesh (Change Data Capture) Project*
