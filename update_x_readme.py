import os

# ==========================================
# CONFIGURATION: X (Twitter) Use Case
# ==========================================
project_config = {
    "title": "X (Twitter): Firehose Filtering & Decoupling",
    "problem_statement": (
        "X processes 500 million posts per day. It is impossible and wasteful for a single "
        "analytics service (e.g., one analyzing 'AI Trends') to ingest *every single post* "
        "about cats, sports, and politics just to find the 1% of data it needs."
    ),
    "requirements": [
        "Ingestion Scale: Handle erratic bursts of traffic (e.g., during the Super Bowl).",
        "Filtering: Discard irrelevant data at the edge (Consumer) before processing.",
        "Decoupling: The 'Sports' service should not crash if the 'Politics' stream spikes.",
        "Latency: Real-time keyword matching."
    ],
    "pipeline_description": (
        "1. **Firehose (Producer)**: A massive, unorganized stream of all public posts.\n"
        "2. **Ingestion**: Kafka Topic `global-firehose` (partitioned by User ID).\n"
        "3. **Filter Engine (Consumer)**: \n"
        "   - Reads the full stream.\n"
        "   - Applies `WHERE topic = 'Tech'` logic.\n"
        "   - Discards 90% of noise silently.\n"
        "4. **Downstream**: Only relevant events are saved to the database."
    ),
    "instructions": "python producer.py"
}

# ==========================================
# LOGIC: README Generator
# ==========================================
def generate_readme():
    target_path = "/workspaces/Taashi_Github/Implementation Code/15_Streaming_Big_Data/05_X_Stream_Filtering"
    
    # 1. Ensure directory exists
    os.makedirs(target_path, exist_ok=True)
    file_path = os.path.join(target_path, "README.md")
    
    # 2. Formatting Helpers
    req_list = "\n".join([f"- {req}" for req in project_config['requirements']])
    B_BASH = "```bash"
    B_END = "```"
    
    # 3. Construct Content
    content = f"""
# {project_config['title']}

## 1. Problem Statement
{project_config['problem_statement']}

## 2. Requirements & KPIs
{req_list}

## 3. Architecture & Pipeline
{project_config['pipeline_description']}

---

## 4. Technical Implementation

### File Structure
- `producer.py`: The "Firehose". Generates a high-speed stream of random topics.
- `consumer.py`: The "Filter". It ignores everything except specific keywords (AI, Tech).
- `utils_logger.py`: Logging config.

### How to Run this Demo

**Step 1: Install Dependencies**
{B_BASH}
pip install -r requirements.txt
{B_END}

**Step 2: Start the Filter Engine (Consumer)**
This service represents a specific team (e.g., the AI Analytics Team) looking for data.
{B_BASH}
python consumer.py
{B_END}
*Observe that it ONLY prints tweets that match its interest profile, ignoring the rest.*

**Step 3: Start the Firehose (Producer)**
This simulates the global stream of all user activity.
{B_BASH}
{project_config['instructions']}
{B_END}
*Warning: This will generate a lot of noise in the terminal to simulate high volume.*

**Step 4: Analyze the Difference**
- The **Producer** terminal is chaos (Sports, K-Pop, Politics).
- The **Consumer** terminal is clean (Only Tech/AI updates).
- This demonstrates how **Stream Filtering** saves compute resources downstream.

---
*Generated by Automation Script | {project_config['title']} Project*
"""

    with open(file_path, "w") as f:
        f.write(content.strip())
    
    print(f"âœ… README successfully written to:\n   {file_path}")

if __name__ == "__main__":
    generate_readme()