import os

# ==========================================
# CONFIGURATION: Netflix Keystone Use Case
# ==========================================
project_config = {
    "title": "Netflix Keystone: Real-Time Data Backbone",
    "problem_statement": (
        "With 200M+ users, Netflix cannot wait for overnight batch jobs to know if "
        "playback is failing. We need a real-time 'Keystone' pipeline to ingest "
        "telemetry (Start, Stop, Buffer) and instantly detect Quality of Experience (QoE) "
        "issues per region or device."
    ),
    "requirements": [
        "High Throughput: Ingest millions of events/second (Heartbeats, Errors).",
        "Sessionization: Correlate 'Start', 'Buffer', and 'Stop' events into a single viewing session.",
        "Latency: Detect buffering loops in < 3 seconds.",
        "Routing: Route data to both Real-time Dashboards (Elasticsearch) and Data Lake (S3)."
    ],
    "pipeline_description": (
        "1. **Edge (Producer)**: Devices emit telemetry events (JSON).\n"
        "2. **Ingest**: Kafka Topic `keystone-telemetry`.\n"
        "3. **Processing (Consumer)**: \n"
        "   - Maintains 'Active Session' state in memory.\n"
        "   - Counts 'Buffer' events per user.\n"
        "4. **Action**: Triggers 'Bitrate Downgrade' alerts if buffering exceeds thresholds."
    ),
    "instructions": "python producer.py"
}

# ==========================================
# LOGIC: README Generator
# ==========================================
def generate_readme():
    target_path = "/workspaces/Taashi_Github/Implementation Code/15_Streaming_Big_Data/03_Netflix_Keystone"
    
    # 1. Ensure directory exists
    os.makedirs(target_path, exist_ok=True)
    file_path = os.path.join(target_path, "README.md")
    
    # 2. Formatting Helpers
    req_list = "\n".join([f"- {req}" for req in project_config['requirements']])
    B_BASH = "```bash"
    B_END = "```"
    
    # 3. Construct Content
    content = f"""
# {project_config['title']}

## 1. Problem Statement
{project_config['problem_statement']}

## 2. Requirements & KPIs
{req_list}

## 3. Architecture & Pipeline
{project_config['pipeline_description']}

---

## 4. Technical Implementation

### File Structure
- `producer.py`: Simulates millions of devices sending playback telemetry.
- `consumer.py`: The Keystone Aggregator. It sessionizes events to calculate view time and buffering.
- `utils_logger.py`: High-performance logging config.

### How to Run this Demo

**Step 1: Install Dependencies**
{B_BASH}
pip install -r requirements.txt
{B_END}

**Step 2: Start the Analytics Engine (Consumer)**
This service watches the stream for quality issues.
{B_BASH}
python consumer.py
{B_END}
*It will simulate a user session where buffering gets critical...*

**Step 3: Start the Traffic Generator (Producer)**
This generates random traffic (Starts, Stops, Heartbeats).
{B_BASH}
{project_config['instructions']}
{B_END}

**Step 4: Observe Real-Time QoE**
Watch the Consumer terminal. You will see:
- â–¶ï¸ **START**: User began a session.
- âš ï¸ **BUFFERING**: Warning logs as lag increases.
- ðŸš¨ **CRITICAL QoE ALERT**: Triggered when a user buffers 3x in a row.
- â¹ï¸ **STOP**: Calculates total viewing duration when session ends.

---
*Generated by Automation Script | {project_config['title']} Project*
"""

    with open(file_path, "w") as f:
        f.write(content.strip())
    
    print(f"âœ… README successfully written to:\n   {file_path}")

if __name__ == "__main__":
    generate_readme()